{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkI7j_b3ccDR"
   },
   "source": [
    "Implementing PCA in Python with Scikit-Learn\n",
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gn4dOWHjccDW"
   },
   "source": [
    "With the availability of high performance CPUs and GPUs, it is pretty much possible to solve every regression, classification, clustering and other related problems using machine learning and deep learning models. However, there are still various factors that cause performance bottlenecks while developing such models. \n",
    "\n",
    "Large number of features in the dataset is one of the factors that affect both the training time as well as accuracy of machine learning models. You have different options to deal with huge number of features in a dataset.\n",
    "\n",
    "1. Try to train the models on original number of features, which take <font color='red'>days or weeks</font> if the number of features is too high.\n",
    "\n",
    "2. Reduce the number of variables by merging correlated variables.\n",
    "\n",
    "3. <font color='green'>Extract the most important features from the dataset that are responsible for maximum variance in the output.</font> Different statistical techniques are used for this purpose e.g. linear discriminant analysis, factor analysis, and <b>principal component analysis (PCA)</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IG6L32-kccDr"
   },
   "source": [
    "About Principal Component Analysis\n",
    "-----------------------------------------------------\n",
    "Principal component analysis, or <b>PCA, is a statistical technique to convert high dimensional data to low dimensional data by selecting the most important features that capture maximum information about the dataset</b>. The features are selected on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal component. \n",
    "\n",
    "The feature that is responsible for second highest variance is considered the second principal component, and so on. It is important to mention that principle components do not have any correlation with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezUIeY3SccDu"
   },
   "source": [
    "Advantages of PCA\n",
    "----------------------------\n",
    "There are two main advantages of dimensionality reduction with PCA.\n",
    "\n",
    "1> The training time of the algorithms reduces significantly with less number of features.\n",
    "\n",
    "2> It is not always possible to analyze data in high dimensions. For instance if there are 100 features in a dataset. Total number of scatter plots required to visualize the data would be (100(100-1))/2 = 4950. Practically it is not possible to analyze data this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yiuwRw2ccDw"
   },
   "source": [
    "Normalization of Features\n",
    "-------------------------------------\n",
    "\n",
    "It is imperative to mention that a feature set must be normalized before applying PCA. For instance if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.\n",
    "\n",
    "Finally, the last point to remember before we start coding is that PCA is a statistical technique and can only be applied to **numeric data. Therefore, categorical features are required to be converted into numerical features before PCA can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEgfQmWgccDz"
   },
   "source": [
    "Important Note\n",
    "---------------------\n",
    ">PCA is a feature Extraction technique. <font color='red'>How ?</font>\n",
    "read this : Say we have 10 independent varaibles. In Feature Extraction we create 10 \"new\" independent variables. However these \"new\" independent variables are created (by PCA) from a combination of each of 10 \"old\" independent varibales. Now the dimensionality reduction comes into action. We keep the most important of the \"new\" independent variables and drop the least important ones. \n",
    "\n",
    "> This technique is <font color='green'>better than simple \"Feature selection\"</font> because each of the \"new\" independent variables is calculated from all the \"old\" independent variables. Hence we are still keeping the most valuable parts of our old variables, even when drop one or more of these \"new\" variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RTvkYQ5ccD2"
   },
   "source": [
    "Implementing PCA with Scikit-Learn\n",
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "yIccLPcXccD5",
    "outputId": "684acf0b-02ca-4470-99dc-564355dd2cef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "# import some data to play with. We are loading the popular Iris Data set\n",
    "irisdata = sns.load_dataset('iris')\n",
    "irisdata.head()  # have a look at the attributres(=> X) and Labels(=> y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUUiElQ6ccEE"
   },
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "X = irisdata.drop('species', axis=1)  \n",
    "y = irisdata['species']\n",
    "\n",
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "\n",
    "#  PCA performs best with a normalized feature set. \n",
    "#  We will perform standard scalar normalization to normalize our feature set.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Performing PCA using Scikit-Learn is a two-step process:\n",
    "# 1. Initialize the PCA class by passing the number of components to the constructor.\n",
    "# 2. Call the fit and then transform method by passing the feature set to these methods. \n",
    "#    The transform method returns the specified number of principal components.\n",
    "\n",
    "# Applying PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()  \n",
    "X_train = pca.fit_transform(X_train)  \n",
    "X_test = pca.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwHEehztccEK"
   },
   "source": [
    "In the code above, we create a PCA object named pca. <font color='green'><b>We did not specify the number of components in the constructor. Hence, all <u>four</u> features in the feature set will be returned for both the training and test sets.</b></font>\n",
    "\n",
    "The PCA class contains `explained_variance_ratio_` which returns the variance caused by each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M9lmowEkccEM",
    "outputId": "9854cbae-2dbc-4870-f975-ba272ea31caf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74223545, 0.21601729, 0.03647121, 0.00527604])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance = pca.explained_variance_ratio_ \n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aK1DU3xrccES"
   },
   "source": [
    "It can be seen that first principal component is responsible for 73 % variance. Similarly, the second principal component causes 21 % variance in the dataset. Collectively we can say that 73 + 21 ~ **95%**  of the classification information contained in the feature set is **captured by the first two principal components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyGqS5kKccEU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01197422]\n",
      " [ 0.29176303]\n",
      " [ 1.27693979]\n",
      " [-2.25154785]\n",
      " [-2.15421304]]\n"
     ]
    }
   ],
   "source": [
    "# Let's first try to use 1 principal component to train our algorithm. \n",
    "# To do so, execute the following code:\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state=100)\n",
    "\n",
    "#  PCA performs best with a normalized feature set. \n",
    "#  We will perform standard scalar normalization to normalize our feature set.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1)  \n",
    "X_train = pca.fit_transform(X_train)  \n",
    "X_test = pca.transform(X_test)  \n",
    "\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "WnmjbfCcccEa",
    "outputId": "0211f450-dba6-4472-c601-4b8266da2b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  5  1]\n",
      " [ 0  1 12]]\n",
      "Accuracy  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Training and Making Predictions\n",
    "# In this case we'll use random forest classification \n",
    "# for making the predictions.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, \n",
    "                                    n_estimators=10)  \n",
    "classifier.fit(X_train, y_train)\n",
    "# Please Note : if n_estimators is not specified in RandomForestClassifier\n",
    "# default value of 10 is taken. For this you may get FutureWarning\n",
    "# To avoid the warning, either specify n_estimators or suppress warnings\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)  \n",
    "\n",
    "\n",
    "# Performance Evaluation\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy ' ,  accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SpGprtCpccEg"
   },
   "source": [
    "It can be seen from the output that with only one feature, the random forest algorithm is able to correctly predict 27 out of 30 instances, resulting in 90% accuracy. **<i><font color='blue'>(This accuracy value will change at train-test dataset changes. So you may get a different accuracy score)</font></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J0JCxh-ccEh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01197422 -1.532126  ]\n",
      " [ 0.29176303 -0.56916401]\n",
      " [ 1.27693979 -1.69685591]\n",
      " [-2.25154785 -0.62953216]\n",
      " [-2.15421304  1.58773001]]\n"
     ]
    }
   ],
   "source": [
    "# Try PCA with 2 Principal Components\n",
    "# pca = PCA(n_components=2)\n",
    "#------------------------------------------\n",
    "# all the above steps would have to be repeated.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state=100)\n",
    "\n",
    "#  PCA performs best with a normalized feature set. \n",
    "#  We will perform standard scalar normalization to normalize our feature set.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)  \n",
    "X_train = pca.fit_transform(X_train)  \n",
    "X_test = pca.transform(X_test)  \n",
    "\n",
    "print(X_train[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dRjNLqy1X-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  4  2]\n",
      " [ 0  5  8]]\n",
      "Accuracy  0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Try PCA with 3 Principal Components\n",
    "# pca = PCA(n_components=3)\n",
    "#------------------------------------------\n",
    "# all the above steps would have to be repeated.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, \n",
    "                                    n_estimators=10)  \n",
    "classifier.fit(X_train, y_train)\n",
    "# Please Note : if n_estimators is not specified in RandomForestClassifier\n",
    "# default value of 10 is taken. For this you may get FutureWarning\n",
    "# To avoid the warning, either specify n_estimators or suppress warnings\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)  \n",
    "\n",
    "\n",
    "# Performance Evaluation\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy ' ,  accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01197422 -1.532126   -0.29982543]\n",
      " [ 0.29176303 -0.56916401  0.05135883]\n",
      " [ 1.27693979 -1.69685591 -0.34542979]\n",
      " [-2.25154785 -0.62953216 -0.24698463]\n",
      " [-2.15421304  1.58773001  0.01212647]]\n"
     ]
    }
   ],
   "source": [
    "# Try PCA with 2 Principal Components\n",
    "# pca = PCA(n_components=2)\n",
    "#------------------------------------------\n",
    "# all the above steps would have to be repeated.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state=100)\n",
    "\n",
    "#  PCA performs best with a normalized feature set. \n",
    "#  We will perform standard scalar normalization to normalize our feature set.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)  \n",
    "X_train = pca.fit_transform(X_train)  \n",
    "X_test = pca.transform(X_test)  \n",
    "\n",
    "print(X_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0  6  0]\n",
      " [ 0  9  4]]\n",
      "Accuracy  0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, \n",
    "                                    n_estimators=10)  \n",
    "classifier.fit(X_train, y_train)\n",
    "# Please Note : if n_estimators is not specified in RandomForestClassifier\n",
    "# default value of 10 is taken. For this you may get FutureWarning\n",
    "# To avoid the warning, either specify n_estimators or suppress warnings\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)  \n",
    "\n",
    "\n",
    "# Performance Evaluation\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "print(cm)  \n",
    "print('Accuracy ' ,  accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YK0-m1_accEm"
   },
   "source": [
    "<font color='red'>Complete your Analysis below : </font>\n",
    "\n",
    "1.  With two principal components the classification accuracy was 76% , as compared to 93% for 1 component.\n",
    "\n",
    "2. With three principal components the classification accuracy again decresaes to 7%.\n",
    "\n",
    "From the above experimentation we achieved optimal level of accuracy while significantly reducing the number of features in the dataset. We saw that accuracy achieved with only 1 principal component is equal to the accuracy achieved with all feature set. It is also pertinent to mention that the accuracy of a classifier doesn't necessarily improve with increased number of principal components. From the results we can see that the accuracy achieved with one principal component => 93% , was greater than the one achieved with two principal components => 76%\n",
    "\n",
    "The number of principal components to retain in a feature set depends on several conditions such as storage capacity, training time, performance, etc. In some dataset all the features are contributing equally to the overall variance, therefore all the principal components are crucial to the predictions and none can be ignored. \n",
    "\n",
    "<font color='green'><b>\n",
    "**A general rule of thumb is to take number of principal components that contribute to significant variance and ignore those with lower variance returns.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "10_Principle Component Analysis_(PCA)_Unsupervised_ML Algo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
